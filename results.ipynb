{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "results_13-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nozuaK3_CKjw"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Neuvork/Engeneering-thesis/blob/master/results.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5jcZnINFcRT"
      },
      "source": [
        "! git clone https://<username>:<password>@github.com/Neuvork/Engeneeringthesis.git --single-branch --branch conv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB_2jKGqwWes"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQIf2yvLB9pn"
      },
      "source": [
        "#DOPISAC CMA\n",
        "#ZROBIC REKURENCYJNY ES\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.linalg import sqrtm\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import copy\n",
        "import cupy as cp\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from Engeneeringthesis.sigmas import Sigmas_Neural_Network\n",
        "from Engeneeringthesis.NeuralNetwork import Neural_Network\n",
        "from Engeneeringthesis.Cma_es import CMA_ES\n",
        "from Engeneeringthesis.Logs import Logs\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl9rVITDlQyH"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU0va8DSXlK5"
      },
      "source": [
        "mempool = cp.get_default_memory_pool()\n",
        "pinned_mempool = cp.get_default_pinned_memory_pool()\n",
        "def cuda_memory_clear():\n",
        "    mempool.free_all_blocks()\n",
        "    pinned_mempool.free_all_blocks()          "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGdtEleAayvp"
      },
      "source": [
        "no_debug = 1\n",
        "basic_debug_mode = 2\n",
        "super_debug_mode = 3\n",
        "only_interesting = 5\n",
        "DEBUG_MODE = only_interesting"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6SmN-lZSUu_"
      },
      "source": [
        "train_ds_mnist = tfds.load(\"mnist\", split = \"train\", shuffle_files=True, batch_size=-1)\n",
        "test_ds_mnist = tfds.load(\"mnist\", split = \"test\", shuffle_files=True, batch_size=-1)\n",
        "\n",
        "train_ds_mnist = tfds.as_numpy(train_ds_mnist)\n",
        "test_ds_mnist = tfds.as_numpy(test_ds_mnist)\n",
        "\n",
        "train_ds_mnist = {\"image\" : cp.array(train_ds_mnist[\"image\"]/255., dtype=cp.float32), \"label\" : cp.array(train_ds_mnist[\"label\"]) }\n",
        "test_ds_mnist = {\"image\" : cp.array(test_ds_mnist[\"image\"]/255., dtype=cp.float32), \"label\" : cp.array(test_ds_mnist[\"label\"]) }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2hy9403NPFa"
      },
      "source": [
        "train_ds_mnist['image'] = train_ds_mnist['image'].reshape((60000, 1, 28, 28))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i2VqaIb-Wyh"
      },
      "source": [
        "def evaluate_population(population, train_ds):\n",
        "    create_input_time = 0\n",
        "    preds_time = 0\n",
        "    points_count_time = 0\n",
        "    j  = 0\n",
        "    if DEBUG_MODE % basic_debug_mode == 0:\n",
        "      print(\"___EVALUATE_POPULATION_START\")\n",
        "    #scores = np.zeros(population.layers[0][1].shape[0], dtype = np.uint32)\n",
        "    scores = cp.zeros(population.population_size, dtype = cp.uint32)\n",
        "    for image, label in zip(cp.array(train_ds['image']), cp.array(train_ds['label'])):\n",
        "        start = time.time()\n",
        "        #image = image.flatten()\n",
        "        #images = cp.zeros(shape = (population.population_size, 1, 28, 28))\n",
        "        #for i in range(population.population_size):\n",
        "        #  images[i] = image\n",
        "        #image = images\n",
        "        create_input_time += time.time() - start\n",
        "        start = time.time()\n",
        "        preds = population.forward(image)\n",
        "        preds_time += time.time() - start\n",
        "        start = time.time()\n",
        "        #scores += cp.asnumpy(preds == label)\n",
        "        scores += preds == label\n",
        "        points_count_time += time.time() - start\n",
        "        j += 1\n",
        "      \n",
        "    if DEBUG_MODE % basic_debug_mode == 0:\n",
        "      print(\"___EVALUATE_POPULATION_STOP\", \"create_input_time: \", create_input_time, \"preds_time:\", preds_time,\n",
        "          \"points_count_time: \", points_count_time, \"\\n best result: \", np.max(cp.asnumpy( scores)),\n",
        "          \"mean socre: \", np.mean(cp.asnumpy( scores)), \"min score: \", np.min(cp.asnumpy( scores))) \n",
        "    if DEBUG_MODE % only_interesting == 0:\n",
        "      print(\"best result: \", np.max(cp.asnumpy( scores)), \"mean socre: \", np.mean(cp.asnumpy( scores)), \"min score: \", np.min(cp.asnumpy( scores)))\n",
        "\n",
        "    return scores"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC4DvHDvSbyJ"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kysKh3qG1ra1"
      },
      "source": [
        "def custom_plot(ax, data):\n",
        "  XD = np.array([1,2,3,4,5])\n",
        "  ax.plot(XD)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i-Agjvi1qPv"
      },
      "source": [
        "logs = Logs([('matrix','covariance'),('vector','sigma'),\n",
        "                      ('vector','isotropic'),('vector','anisotropic'),('vector','mean'),\n",
        "                      ('number','best-score')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwXWMRBrjoj2"
      },
      "source": [
        "logs = Logs([('matrix','covariance'),('population', 'population'),('number','sigma'),\n",
        "                      ('vector','isotropic'),('vector','anisotropic'),('vector','mean'),\n",
        "                      ('number','best-score'), ('vector', 'mean_act - mena_prev')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G-io99VNezd"
      },
      "source": [
        "dimensionalities = []\n",
        "for i in range(784):\n",
        "  dimensionalities.append(10)\n",
        "number_of_cages = 784"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuQqT9Jcmqvk"
      },
      "source": [
        "import cupy as cp\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "from Engeneeringthesis.kernels import dot_cuda_paralell, max_pooling_cuda_paralell, convolve_cuda_paralell, dot_cuda_paralell_many_inputs, convolve_cuda_paralell_many_inputs\n",
        "no_debug = 1\n",
        "basic_debug_mode = 2\n",
        "super_debug_mode = 3\n",
        "only_interesting = 5\n",
        "DEBUG_MODE = only_interesting\n",
        "from Engeneeringthesis.kernels import dot_cuda_paralell, max_pooling_cuda_paralell, convolve_cuda_paralell\n",
        "class Neural_Network:\n",
        "\n",
        "\n",
        "  def __init__(self,num_nets,input_size,given_layers,loc=0,scale=.3, cage_dimensionalities = None):#after init in neuronized state\n",
        "    self.mempool = cp.get_default_memory_pool()\n",
        "    self.pinned_mempool = cp.get_default_memory_pool()\n",
        "    self.population_size = num_nets\n",
        "    self.input_size = input_size\n",
        "    self.input_layers = given_layers\n",
        "    self.vectorized = False #if NN is in state of being vectorized or neuronized\n",
        "    self.layers = [] #empty if in vectorized,neural network if in neuronized\n",
        "    self.matrix = None #empty if in neuronized, vector if in vectorized\n",
        "    self.layers_shapes = self.parse_input(given_layers,input_size,num_nets) #remember the shape of network,and parse user input\n",
        "    self.dimensionality = self.compute_dimensionality()\n",
        "    self.cage_dimensionalities = cage_dimensionalities\n",
        "    for layer in self.layers_shapes:\n",
        "      if layer[0] == 'conv':\n",
        "        self.layers.append(['conv', cp.random.normal(loc = loc, scale = scale, size = layer[1]).astype(cp.float32)])   #layer[0] -> conv ; layer[1] ->[num_nets, out_channel, in_channel, filter_wdth, filter_height]\n",
        "      if layer[0] == 'linear':\n",
        "        self.layers.append(['linear', cp.random.normal(loc = loc, scale = scale, size = layer[1]).astype(cp.float32)]) \n",
        "      if layer[0] == 'bias':\n",
        "        self.layers.append(['bias', cp.zeros(shape = layer[1]).astype(cp.float32)])\n",
        "  \n",
        "\n",
        "  def cuda_memory_clear(self):\n",
        "    self.mempool.free_all_blocks()\n",
        "    self.pinned_mempool.free_all_blocks()          \n",
        "\n",
        "  def parse_to_vector(self): # every individual is getting trapnsfered to vector\n",
        "    ret_mat = np.zeros((self.population_size, self.dimensionality),dtype = np.float32)\n",
        "    index = 0\n",
        "    for layer in self.layers:\n",
        "      i = 0\n",
        "      for individual in layer[1]:\n",
        "        ret_mat[i][index:index+individual.size] = cp.asnumpy(individual.flatten())\n",
        "        i += 1 \n",
        "      index += layer[1][0].flatten().size\n",
        "    self.layers = []\n",
        "    self.cuda_memory_clear()\n",
        "    self.matrix = cp.array(ret_mat, dtype = cp.float32)\n",
        "    self.vectorized = True\n",
        "\n",
        "  def list_memory_clear(self, lista):\n",
        "    for i in range(len(lista)):\n",
        "      del lista[0]\n",
        "\n",
        "  def parse_input(self,given_layers,input_size,num_nets):\n",
        "    layers = []\n",
        "    input_size = (input_size[0],input_size[1],input_size[2])\n",
        "\n",
        "    iterator = 1\n",
        "    for layer in given_layers:\n",
        "      print(layer, input_size, type(input_size))\n",
        "\n",
        "      if layer[0] == 'conv':\n",
        "        layers.append((layer[0],[num_nets,layer[1][0],input_size[0],layer[1][1],layer[1][2]]))\n",
        "        input_size = (layer[1][0],input_size[1]-layer[1][1]+1,input_size[2]-layer[1][2]+1)\n",
        "        input_size = (input_size[0],np.ceil(input_size[1]/2),np.ceil(input_size[2]/2))\n",
        "        input_size = tuple(map(lambda x:int(x), input_size))\n",
        "\n",
        "      if layer[0] == 'linear':\n",
        "        temp = 1\n",
        "        if type(input_size) == int:\n",
        "          temp = input_size\n",
        "        else:\n",
        "          temp = reduce( lambda a,b: a*b, input_size)\n",
        "        input_size = int(temp)\n",
        "        layers.append((layer[0],[num_nets,input_size,layer[1]]))\n",
        "        input_size = layer[1]\n",
        "\n",
        "      if iterator != len(given_layers):\n",
        "        layers.append(('bias', [num_nets] + list(input_size)))\n",
        "      iterator += 1\n",
        "      \n",
        "      \n",
        "    print(\"layers: \", layers)\n",
        "    return layers\n",
        "  \n",
        "  def compute_dimensionality(self):\n",
        "    number_of_weights = 0\n",
        "    for layer_shape in self.layers_shapes:\n",
        "      print(\"layer_shape: \", layer_shape)\n",
        "      weights_in_layer = reduce(lambda a,b: a*b, layer_shape[1][1:])\n",
        "      number_of_weights += weights_in_layer\n",
        "    return number_of_weights\n",
        "\n",
        "\n",
        "\n",
        "  def sample(self,B,D, sigma, mean, lam):\n",
        "    self.layers = [] #cleaning previous population\n",
        "    self.cuda_memory_clear()\n",
        "    #concat sampled vectors and parse them\n",
        "    ret_mat = cp.zeros((lam, self.dimensionality),dtype = cp.float32)\n",
        "    #L = cp.linalg.cholesky(covariance_matrix*(sigma**2)).astype(cp.float32)\n",
        "    for i in range(lam):\n",
        "      ret_mat[i] = self.multivariate_cholesky(mean,B,D,sigma)\n",
        "      #ret_mat[i] = cp.random.multivariate_normal(mean, covariance_matrix * (sigma**2))\n",
        "      self.cuda_memory_clear()\n",
        "    self.matrix = ret_mat\n",
        "    self.vectorized = True\n",
        "\n",
        "  def multivariate_cholesky(self,mean,B,D,sigma):\n",
        "    vector = cp.random.normal(loc = 0,scale = 1,size = self.dimensionality,dtype = cp.float32)\n",
        "    ret_val = sigma*B.dot(D*vector) + mean\n",
        "    return ret_val\n",
        "\n",
        "  def caged_sample(self,covariance_matrices, sigmas, means, lam):\n",
        "    self.layers = [] #cleaning previous population\n",
        "    self.cuda_memory_clear()\n",
        "    #concat sampled vectors and parse them\n",
        "    ret_mat = cp.zeros((lam, self.dimensionality),dtype = cp.float32)\n",
        "    L = []\n",
        "    for i in range(len(self.cage_dimensionalities)):\n",
        "      L.append(cp.linalg.cholesky(covariance_matrices[i]*(sigmas[i]**2)).astype(cp.float32))\n",
        "    for i in range(lam):\n",
        "      ret_mat[i] = self.caged_multivariate_cholesky(means,L)\n",
        "    self.cuda_memory_clear()\n",
        "    self.matrix = ret_mat\n",
        "    self.vectorized = True\n",
        "  \n",
        "  def caged_multivariate_cholesky(self, means, cholesky_covariances):\n",
        "    vector = cp.array([])\n",
        "    for i in range(len(means)):\n",
        "      sampled_vector = cp.random.normal(loc = 0,scale = 1,size = cholesky_covariances[i].shape[0],dtype = cp.float32)\n",
        "      vector = cp.concatenate((vector, cholesky_covariances[i].dot(sampled_vector) + means[i]))\n",
        "    return vector\n",
        "\n",
        "  def mult(self, l):\n",
        "    ret_val = 1\n",
        "    for number in l:\n",
        "      ret_val *= number\n",
        "    return ret_val\n",
        "\n",
        "  def parse_from_vectors(self):\n",
        "    numbers = []\n",
        "    self.matrix = cp.asnumpy(self.matrix)\n",
        "    self.cuda_memory_clear()\n",
        "    for layer in self.layers_shapes:\n",
        "      print(layer[1])\n",
        "      numbers.append(self.mult(layer[1][1:]))\n",
        "    start = 0\n",
        "    it = 0\n",
        "    for number in numbers:\n",
        "      self.layers.append((self.layers_shapes[it][0],cp.array(self.matrix[:,start:(start+number)]).reshape(self.layers_shapes[it][1])))\n",
        "      it+=1\n",
        "      start += number\n",
        "    self.matrix = None\n",
        "    self.vectorized = False\n",
        "\n",
        "  def move_to_cpu(self):\n",
        "    for layer in self.layers:\n",
        "      layer[1] = cp.asnumpy(layer[1])\n",
        "\n",
        "  def move_to_gpu(self):\n",
        "    for layer in self.layers:\n",
        "      layer[1] = cp.array(layer[1])\n",
        "\n",
        "  def forward(self, state):\n",
        "    layer_num = 0\n",
        "    temp = state.copy()\n",
        "    first_lin = 0\n",
        "    for layer in self.layers:\n",
        "      if layer[0]=='conv':\n",
        "        if layer_num == 0:\n",
        "          temp = convolve_cuda_paralell(temp, layer[1])\n",
        "        else:\n",
        "          temp = convolve_cuda_paralell_many_inputs(temp, layer[1])\n",
        "        temp = max_pooling_cuda_paralell(temp)\n",
        "      if layer[0]=='linear':\n",
        "        if first_lin == 0:\n",
        "          first_lin+=1\n",
        "          temp = temp.reshape(-1,layer[1].shape[1])\n",
        "        if layer_num ==0:\n",
        "          temp = dot_cuda_paralell(temp, layer[1])\n",
        "        else:\n",
        "          temp = dot_cuda_paralell_many_inputs(temp, layer[1])\n",
        "      if layer[0] == 'bias':\n",
        "        temp += layer[1]\n",
        "        temp = cp.tanh(temp, dtype = cp.float32)\n",
        "      layer_num += 1\n",
        "    return cp.argmax(temp, axis = 1)\n",
        "\n",
        "  def replace_individual(self, i, individual):\n",
        "    i = int(i)\n",
        "    for j in range(len(self.layers)):\n",
        "      self.layers[j][1][i] = individual[j]\n",
        "      self.list_memory_clear(individual)\n",
        "    del individual\n",
        "\n",
        "#self.cage dimensionalities \n",
        "  def return_chosen_ones(self, indices, number_of_cage = None):\n",
        "    if not self.vectorized:\n",
        "        self.parse_to_vector()\n",
        "    \n",
        "    if number_of_cage == None:\n",
        "      return self.matrix[indices]\n",
        "    else:\n",
        "      begin = self.cage_dimensionalities[:number_of_cage].sum()\n",
        "      move = self.cage_dimensionalities[number_of_cage]\n",
        "      return self.matrix[indices, begin : begin + move]\n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def get_individual(self, i):\n",
        "    result_individual = []\n",
        "    i = int(i)\n",
        "    for layer in self.layers:\n",
        "      result_individual.append(layer[1][i].copy())\n",
        "    return result_individual\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2R8qHtuQd1A"
      },
      "source": [
        "POPULATION_SIZE = 64\n",
        "#input size do zmiany\n",
        "population = Neural_Network(POPULATION_SIZE,  (1, 28, 28), \n",
        "                            [\n",
        "                             ['conv', (1, 3, 3), [1.,1.]],\n",
        "                             ['linear', 10, [1.,1.]]\n",
        "                             ],\n",
        "                            cage_dimensionalities=np.array(dimensionalities))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px2h9D5iCFwZ"
      },
      "source": [
        "logs = Logs([('matrix','covariance'),('population', 'population'),('number','sigma'),\n",
        "                      ('vector','isotropic'),('vector','anisotropic'),('vector','mean'),\n",
        "                      ('number','best-score'), ('vector', 'mean_act - mena_prev')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZOmRD_nICsz"
      },
      "source": [
        "classifier = CMA_ES(population, .01, evaluate_population, logs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2dgy2JkII5_"
      },
      "source": [
        "classifier.fit(train_ds_mnist, POPULATION_SIZE//2, POPULATION_SIZE, 500)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}