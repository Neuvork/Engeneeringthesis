{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "results.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nozuaK3_CKjw"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Neuvork/Engeneering-thesis/blob/master/results.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCs4hqNaNKk1"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKk8gqGsRibt"
      },
      "source": [
        "# This notebook is only example of use of our library to perform evolution on neural networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQIf2yvLB9pn"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from Engeneeringthesis.kernels import *\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import copy\n",
        "import cupy as cp\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from Engeneeringthesis.NeuralNetwork import Neural_Network\n",
        "from Engeneeringthesis.Cma_es import CMA_ES\n",
        "from Engeneeringthesis.Caged_CMA_ES import Caged_CMA_ES\n",
        "from Engeneeringthesis.Logs import Logs\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU0va8DSXlK5"
      },
      "source": [
        "mempool = cp.get_default_memory_pool()\n",
        "pinned_mempool = cp.get_default_pinned_memory_pool()\n",
        "def cuda_memory_clear():\n",
        "    mempool.free_all_blocks()\n",
        "    pinned_mempool.free_all_blocks()          "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk4blBpGJmhK"
      },
      "source": [
        "## Dataset preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiPNoAcKJjU1"
      },
      "source": [
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    'cifar10',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")\n",
        "\n",
        "\n",
        "def normalize_img(image, label):\n",
        "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
        "  return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "ds_train = ds_train.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "ds_train = ds_train.cache()\n",
        "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
        "ds_train = ds_train.batch(32)\n",
        "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "ds_test = ds_test.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "ds_test = ds_test.batch(32)\n",
        "ds_test = ds_test.cache()\n",
        "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q_rCLbdLaPM"
      },
      "source": [
        "#ensuring that algorithm won't see any images that tensorflow haven't seen\n",
        "TRAINING_SIZE = 0 #global value for division in evaluate function\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for batch in ds_train:\n",
        "  for image, label in zip(batch[0], batch[1]):\n",
        "    #for rgb images:\n",
        "    temp = image.numpy().copy()\n",
        "    image = image.numpy().reshape(image.shape[2], image.shape[0], image.shape[1])\n",
        "    if image.shape[0] == 3:\n",
        "      image[0, :, :] = temp[:,:,0]\n",
        "      image[1, :, :] = temp[:,:,1]\n",
        "      image[2, :, :] = temp[:,:,2]\n",
        "    images.append(cp.array(image, dtype =cp.float32))\n",
        "    labels.append(label)\n",
        "    TRAINING_SIZE += 1\n",
        "\n",
        "for batch in ds_test:\n",
        "  for image, label in zip(batch[0], batch[1]):\n",
        "    #for rgb images:\n",
        "    temp = image.numpy().copy()\n",
        "    image = image.numpy().reshape(image.shape[2], image.shape[0], image.shape[1])\n",
        "    if image.shape[0] == 3:\n",
        "      image[0, :, :] = temp[:,:,0]\n",
        "      image[1, :, :] = temp[:,:,1]\n",
        "      image[2, :, :] = temp[:,:,2]\n",
        "    images.append(cp.array(image, dtype =cp.float32))\n",
        "    labels.append(label)\n",
        "images = cp.array(images, dtype = cp.float32)\n",
        "labels = cp.array(labels)\n",
        "print(images.shape)\n",
        "train_ds_mnist = {\"image\" : images, \"label\" : labels }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsU0WaHo_Q7g"
      },
      "source": [
        "## Training network with TensorFlow\n",
        "Of course there is no need to pretrain network, epecially when evaluate function cannot be derivated it is impoosible to perform previous training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Jr2RJuS0Ilg"
      },
      "source": [
        "model = tf.keras.models.Sequential([                    \n",
        "  tf.keras.layers.Conv2D(filters= 4, kernel_size = 3, activation='tanh', use_bias=False),\n",
        "  tf.keras.layers.MaxPool2D(),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(10, use_bias=False)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.0008),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    ds_train,\n",
        "    epochs=20,\n",
        "    validation_data=ds_test,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvXdRlTPLRV7"
      },
      "source": [
        "## Alocating population\n",
        "We tested our algorithm on network with no bias after convolutional layer and with structirue of:\n",
        "\n",
        "\n",
        "\n",
        "1.   Convolutional layer with 4 filters\n",
        "2.   Dense layer\n",
        "\n",
        "### Important\n",
        "Do not use copy conv or copy linear in Neural Networks made of combinations different than conv and linear or just linear, these parsers are not adapted to copy from TensorFlow different kinds of neural networks. Also set use_bias to False when creating population.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dapKXAOa8gj9"
      },
      "source": [
        "POPULATION_SIZE = 2048\n",
        "input_size = train_ds_mnist['image'][0].shape\n",
        "population = Neural_Network(POPULATION_SIZE,  input_size, \n",
        "                            [\n",
        "                             ['conv', (4, 3, 3), [1.,1.]],\n",
        "                             ['linear', 10, [1.,1.]]\n",
        "                             ],\n",
        "                             use_bias=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieJK8l9LEOrS"
      },
      "source": [
        "## Measure of individuals in population for algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBsR5kwYEQOO"
      },
      "source": [
        "def evaluate_population(population, train_ds):\n",
        "    global TRAINING_SIZE\n",
        "    create_input_time = 0\n",
        "    preds_time = 0\n",
        "    points_count_time = 0\n",
        "    j  = 0\n",
        "    train_dataset = {'image' : train_ds['image'][:TRAINING_SIZE],\n",
        "                      'label' : train_ds['label'][:TRAINING_SIZE] }\n",
        "    validation_dataset = {'image' : train_ds['image'][TRAINING_SIZE:],\n",
        "                      'label' : train_ds['label'][TRAINING_SIZE:] }\n",
        "    train_scores = cp.zeros(population.population_size, dtype = cp.uint32)\n",
        "    i = 0\n",
        "    for image, label in zip(cp.array(train_dataset['image']), cp.array(train_dataset['label'])):\n",
        "        if i % 10 == 0:\n",
        "            clear_output()\n",
        "            print(\"Train dataset done in: \" + str(i/len(train_dataset['image'])) + \"%\")\n",
        "        i+=1\n",
        "        start = time.time()\n",
        "        create_input_time += time.time() - start\n",
        "        start = time.time()\n",
        "        preds = population.forward(image)\n",
        "        preds_time += time.time() - start\n",
        "        start = time.time()\n",
        "        train_scores += preds == label\n",
        "        points_count_time += time.time() - start\n",
        "        j += 1\n",
        "\n",
        "    validation_scores = cp.zeros(population.population_size, dtype = cp.uint32)\n",
        "    i = 0\n",
        "    for image, label in zip(cp.array(validation_dataset['image']), cp.array(validation_dataset['label'])):\n",
        "        if i % 10 == 0:\n",
        "            clear_output()\n",
        "            print(\"Validation dataset done in: \" + str(i/len(train_dataset['image'])) + \"%\")\n",
        "        i+=1\n",
        "        start = time.time()\n",
        "        create_input_time += time.time() - start\n",
        "        start = time.time()\n",
        "        preds = population.forward(image)\n",
        "        preds_time += time.time() - start\n",
        "        start = time.time()\n",
        "        validation_scores += preds == label\n",
        "        points_count_time += time.time() - start\n",
        "        j += 1\n",
        "      \n",
        "\n",
        "    return train_scores/len(train_dataset['image']), validation_scores/len(validation_dataset['image'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAPb2DkP_5aK"
      },
      "source": [
        "# Parsers of layers from TensorFlow to population\n",
        " Due to differences in implenetation of convolutional layers we needed to implement methods that will transfer layers from one object to antoher, for tests we used only one convolutional layer with 4 filters and dense layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKbhkB2s2Ozy"
      },
      "source": [
        "def copy_conv_layer(model_layer_num, population_layer_num, individual_num=0):\n",
        "  global model\n",
        "  global population\n",
        "  model_layer = model.layers[model_layer_num].weights[0]\n",
        "  for output_filter_number in range(population.layers[population_layer_num][1].shape[1]):\n",
        "    for input_filter_number in range(population.layers[population_layer_num][1].shape[2]):\n",
        "      population.layers[population_layer_num][1][individual_num, output_filter_number, input_filter_number, :, :] = cp.array(model_layer[:, :, input_filter_number, output_filter_number].numpy(), dtype=cp.float32)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hvLcA2864-6"
      },
      "source": [
        "def inv(perm):\n",
        "    inverse = [0] * len(perm)\n",
        "    for i, p in enumerate(perm):\n",
        "        inverse[p] = i\n",
        "    return inverse\n",
        "\n",
        "\n",
        "def copy_linear_layer(model_layer_num, population_layer_num, individual_num=0):\n",
        "  global model\n",
        "  global population\n",
        "  model_layer = model.layers[model_layer_num].weights[0].numpy()\n",
        "  population_layer = cp.zeros(shape=model_layer.shape, dtype = cp.float32)\n",
        "  prev_shape = population.input_sizes[population_layer_num -1]\n",
        "  s1 = prev_shape[2]\n",
        "  s2 = prev_shape[3]\n",
        "  permutation = np.zeros(shape = (population_layer.shape[0]), dtype = np.int32)\n",
        "  for i in range(prev_shape[1]): \n",
        "    for j in range(prev_shape[2]): \n",
        "      for k in range(prev_shape[3]): \n",
        "        permutation[j * prev_shape[2] * prev_shape[1] + k * prev_shape[1] + i] = i*s1*s2 + j*s2 + k\n",
        "\n",
        "\n",
        "  inverted = inv(permutation)\n",
        "  for i in range(len(inverted)):\n",
        "    population_layer[i,:] = cp.array(model_layer[inverted[i], :], dtype=cp.float32)\n",
        "\n",
        "  population.layers[population_layer_num][1][individual_num] = population_layer"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKcefh7KEm2B"
      },
      "source": [
        "for i in range(20):\n",
        "  copy_conv_layer(0, 0, i)\n",
        "  copy_linear_layer(3, 2, i)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fr56M2lAS5n"
      },
      "source": [
        "## Ensuring that results in both models are the same (test of parsers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXqdmXwhvF8G"
      },
      "source": [
        "model.evaluate(\n",
        "    ds_train\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVT810GJvbKl"
      },
      "source": [
        "population_score = evaluate_population(population,train_ds_mnist)\n",
        "cp.max(population_score[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuE3cQi-JIpi"
      },
      "source": [
        "### Important\n",
        "Sometimes pyplot change scale of plots, so for example if after few iterations covariance matrix values are from range $[-0.0001,0.0001]$ and then they suddenly jump to range $[-10,10]$ check if scale did not change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFFqDZU2Q7F_"
      },
      "source": [
        "logs = Logs([('matrix','covariance'),('population', 'population'),('number','sigma'),\n",
        "                      ('vector','isotropic'),('vector','anisotropic'),('vector','mean'),\n",
        "                      ('number','best-train-score'), ('number','best-validation-score'),\n",
        "                       ('vector', 'mean_act - mena_prev')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW0GxRCgAZxf"
      },
      "source": [
        "## Racing with Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5qo8zHYKwBc"
      },
      "source": [
        "### Important\n",
        "We do not recommend playing with hyperparameters of CMA_ES init and fit function call, they are picked by us to show good results, and are also found by running numerous experiments so we can not promise that picking other hyperparameters will also yield good results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF8NUXjXIeOy"
      },
      "source": [
        "classifier = CMA_ES(population, .01, evaluate_population, logs, hp_loops_number=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX4CLDUmQ-r-"
      },
      "source": [
        "classifier.fit(train_ds_mnist, POPULATION_SIZE//64, POPULATION_SIZE, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXjm-Q84F-6c"
      },
      "source": [
        "## MNIST from zero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI5YwVIhGUzG"
      },
      "source": [
        "train_ds_mnist = tfds.load(\"mnist\", split = \"train\", shuffle_files=True, batch_size=-1)\n",
        "test_ds_mnist = tfds.load(\"mnist\", split = \"test\", shuffle_files=True, batch_size=-1)\n",
        "\n",
        "train_ds_mnist = tfds.as_numpy(train_ds_mnist)\n",
        "test_ds_mnist = tfds.as_numpy(test_ds_mnist)\n",
        "\n",
        "train_ds_mnist = {\"image\" : cp.array(train_ds_mnist[\"image\"]/255., dtype=cp.float32), \"label\" : cp.array(train_ds_mnist[\"label\"]) }\n",
        "test_ds_mnist = {\"image\" : cp.array(test_ds_mnist[\"image\"]/255., dtype=cp.float32), \"label\" : cp.array(test_ds_mnist[\"label\"]) }\n",
        "train_ds_mnist['image'] = train_ds_mnist['image'].reshape((60000, 1, 28, 28))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edUQAWRGGD4y"
      },
      "source": [
        "POPULATION_SIZE = 2048\n",
        "input_size = train_ds_mnist['image'][0].shape\n",
        "population = Neural_Network(POPULATION_SIZE,  input_size, \n",
        "                            [\n",
        "                             ['linear', 10, [1.,1.]]\n",
        "                             ],\n",
        "                             use_bias=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh7T6O9XIYy5"
      },
      "source": [
        "We do not recommend to change logs. Of course You can do it, but then You will need to change loging in CMA-ES file (inside fit function). Also please be aware that after changing file from library in colaboratory, you will need to reset runtime so this file will be correctly imported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8WaefqxGIbv"
      },
      "source": [
        "logs = Logs([('matrix','covariance'),('population', 'population'),('number','sigma'),\n",
        "                      ('vector','isotropic'),('vector','anisotropic'),('vector','mean'),\n",
        "                      ('number','best-train-score'), ('number','best-validation-score'),\n",
        "                       ('vector', 'mean_act - mena_prev')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIbQlhPcGLvY"
      },
      "source": [
        "classifier = CMA_ES(population, .11, evaluate_population, logs, param_dimensionality = 21000)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efz5n_gCIL5Q"
      },
      "source": [
        "Be patient when waiting for plots, MNIST should work relatively fast, but CIFAR10 is slow. Expect that one iteration of algorithm will take few minutes. Plots are generated after each iteration, You need to scrool output down to see all of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xf6ZrOoZGQZX"
      },
      "source": [
        "classifier.fit(train_ds_mnist, POPULATION_SIZE//64, POPULATION_SIZE, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL3-rd1RGjC5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}